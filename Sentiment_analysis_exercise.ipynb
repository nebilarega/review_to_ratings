{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment_analysis_exercise.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1tUM8KFRs-XSMDJzsr_DI6r81o94zjLc9",
      "authorship_tag": "ABX9TyNH6jqeZXdW/+tP2GKuBhRq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nebilarega/review_to_ratings/blob/main/Sentiment_analysis_exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euJoQxT_HJZl"
      },
      "source": [
        "from six.moves import urllib\r\n",
        "import tensorflow as tf\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import pandas as pd\r\n",
        "import os\r\n",
        "import tarfile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbYUJV63H5wU"
      },
      "source": [
        "# To have the data permanently mount drive\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNqcb2PfIqwB"
      },
      "source": [
        "# Get the data and move the rating and review text in two files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaOJP3kdKfn4"
      },
      "source": [
        "## fetching and extracting the data to the wanted path"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWMOSd-oIs-a"
      },
      "source": [
        "def download_progress(count, block_size, total_size):\r\n",
        "    percent = count * block_size * 100 // total_size\r\n",
        "    sys.stdout.write(\"\\rDownloading: {}%\".format(percent))\r\n",
        "    sys.stdout.flush()\r\n",
        "\r\n",
        "def fetch_data(url, file_path, extracted_path):\r\n",
        "  file_name = file_path+'/mul_domain_dataset.tar.gz'\r\n",
        "  urllib.request.urlretrieve(url, filename=file_name, reporthook=download_progress)\r\n",
        "  tar = tarfile.open(file_name)\r\n",
        "  tar.extractall(extracted_path)\r\n",
        "  tar.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpgtPPSaMKFN"
      },
      "source": [
        "file_path = '/content/drive/MyDrive/multi_domain_dataset'\r\n",
        "url = 'http://www.cs.jhu.edu/~mdredze/datasets/sentiment/unprocessed.tar.gz'\r\n",
        "# fetch the data and extract it\r\n",
        "fetch_data(url, file_path, file_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_SYoSTUKvA2"
      },
      "source": [
        "# Get the necessary data from the list of directories"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvQ9MRtMK6vP"
      },
      "source": [
        "## get the directory list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l69EF_ojmMsk"
      },
      "source": [
        "# list the directory where the extracted directories are\r\n",
        "main_path = '/content/drive/MyDrive/multi_domain_dataset/sorted_data/'\r\n",
        "dir_list = os.listdir(main_path)\r\n",
        "\r\n",
        "# if there are other files that are not directories filter them out\r\n",
        "for i in range(len(dir_list)-1):\r\n",
        "  dir = os.path.join(main_path,dir_list[i])\r\n",
        "  if not os.path.isdir(dir):\r\n",
        "    dir_list.pop(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfhN4GDuNLuZ"
      },
      "source": [
        "## Separate the necessary features from the given data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVS5a-qPNoKB"
      },
      "source": [
        "### the data holds many features separted by opend and closed tab. I extract the features that are between  <review_text> </review_text> and <ratings> </ratings>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOXzIPeJQcWa"
      },
      "source": [
        "file1 = '/content/drive/MyDrive/multi_domain_dataset/review_text'\r\n",
        "file2 = '/content/drive/MyDrive/multi_domain_dataset/ratings'\r\n",
        "\r\n",
        "def separate_text_ratings(path, str1, str2):\r\n",
        "  with open(path, 'a+', encoding='ISO-8859-1') as f_re_w:\r\n",
        "    for rev_path in dir_list:\r\n",
        "      all_review = main_path + rev_path + '/all.review'\r\n",
        "      intermidate_value = open(all_review, 'r', encoding='ISO-8859-1')\r\n",
        "      line_length = len(intermidate_value.readlines())\r\n",
        "      intermidate_value.close()\r\n",
        "      with open(all_review, 'r', encoding='ISO-8859-1') as f_r:\r\n",
        "        for file_ in range(line_length):\r\n",
        "          if f_r.readline().find(str1) != -1:\r\n",
        "            written_line = ''\r\n",
        "            next_line = f_r.readline()\r\n",
        "            while next_line.find(str2) == -1:\r\n",
        "              written_line = written_line + ' ' + next_line.rstrip('\\n')\r\n",
        "              next_line = f_r.readline()\r\n",
        "            f_re_w.write(written_line+'\\n')\r\n",
        "        print('finished file', rev_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJp6oouYMte2"
      },
      "source": [
        "# Separate the review text and save them in a separate file\r\n",
        "separate_text_ratings(file1, '<review_text>', '</review_text>')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSKV4jzc3NYa"
      },
      "source": [
        "# Separate the ratings and save them in a separate file\r\n",
        "separate_text_ratings(file2,'<rating>', '</rating>')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-l-VZP7pf3c"
      },
      "source": [
        "# Save the total lines in the files, which are the same so just use one of the files\r\n",
        "with open(file1, 'r', encoding='ISO-8859-1') as f:\r\n",
        "  dataset_size = len(f.readlines())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-c0tqpk-PJS-"
      },
      "source": [
        "## Check the level of data imbalance and balance it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PEWGKy_Pa9F"
      },
      "source": [
        "### Count the occurance of the five ratings and get the line they occured"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MWdrK4ypa48"
      },
      "source": [
        "# This below is an inefficient way to find occurance\r\n",
        "ones, twos, threes, fours, fives = 0,0,0,0,0  # counting lists\r\n",
        "one_line, two_line, three_line, four_line, five_line = [],[],[],[],[] # occurance lines\r\n",
        "with open(file2, 'r', encoding='ISO-8859-1') as f:\r\n",
        "  for i in range(dataset_size):\r\n",
        "    line = f.readline()\r\n",
        "    if float(line.rstrip('\\n').lstrip(' ')) == 1.0:\r\n",
        "      ones += 1\r\n",
        "      one_line.append(i)\r\n",
        "    elif float(line.rstrip('\\n').lstrip(' ')) == 2.0:\r\n",
        "      twos += 1\r\n",
        "      two_line.append(i)\r\n",
        "    elif float(line.rstrip('\\n').lstrip(' ')) == 3.0:\r\n",
        "      threes += 1\r\n",
        "      three_line.append(i)\r\n",
        "    elif float(line.rstrip('\\n').lstrip(' ')) == 4.0:\r\n",
        "      fours += 1\r\n",
        "      four_line.append(i)\r\n",
        "    elif float(line.rstrip('\\n').lstrip(' ')) == 5.0:\r\n",
        "      fives += 1\r\n",
        "      five_line.append(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-VfaO_Bz0oW",
        "outputId": "06512608-1cbf-4031-f76d-76588704ee39"
      },
      "source": [
        "print('num of ones', ones)\r\n",
        "print('num of twos', twos)\r\n",
        "print('num of threes', threes)\r\n",
        "print('num of fours', fours)\r\n",
        "print('num of fives', fives)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num of ones 103953\n",
            "num of twos 80278\n",
            "num of threes 0\n",
            "num of fours 320681\n",
            "num of fives 917618\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lt6MYjNQjd_"
      },
      "source": [
        "### As we can see from the above the classes are highly imbalanced. There is no 3 rating even. Thus we must balance the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZ7AdNt0qGgE"
      },
      "source": [
        "# using the lines we will randomly select 80000 reviews from each class and save it to have a balanced file\r\n",
        "file_b1 = '/content/drive/MyDrive/multi_domain_dataset/balanced_review_text'\r\n",
        "file_b2 = '/content/drive/MyDrive/multi_domain_dataset/balanced_ratings'\r\n",
        "\r\n",
        "def balanced_classes(one_line=one_line, two_line=two_line, four_line=four_line, five_line=five_line):\r\n",
        "  shuffle_index_1 = np.random.permutation(len(one_line))\r\n",
        "  shuffle_index_2 = np.random.permutation(len(two_line))\r\n",
        "  shuffle_index_4 = np.random.permutation(len(four_line))\r\n",
        "  shuffle_index_5 = np.random.permutation(len(five_line))\r\n",
        "\r\n",
        "  one_line = np.array(one_line)\r\n",
        "  two_line = np.array(two_line)\r\n",
        "  four_line = np.array(four_line)\r\n",
        "  five_line = np.array(five_line)\r\n",
        "\r\n",
        "  acc_lines = []\r\n",
        "  acc_lines = np.append(acc_lines, one_line[shuffle_index_1][:80000])\r\n",
        "  acc_lines = np.append(acc_lines, two_line[shuffle_index_2][:80000])\r\n",
        "  acc_lines = np.append(acc_lines, four_line[shuffle_index_4][:80000])\r\n",
        "  acc_lines = np.append(acc_lines, five_line[shuffle_index_5][:80000])\r\n",
        "\r\n",
        "  with open(file1, 'r', encoding='ISO-8859-1') as f1, open(file2, 'r', encoding='ISO-8859-1') as f2, open(file_b1, 'w',encoding='ISO-8859-1') as fb1, open(file_b2, 'w', encoding='ISO-8859-1') as fb2:\r\n",
        "    for index in range(dataset_size):\r\n",
        "      review = f1.readline()\r\n",
        "      rating = f2.readline()\r\n",
        "      if index in acc_lines:\r\n",
        "        fb1.write(review)\r\n",
        "        fb2.write(rating)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RItshOIpFWX"
      },
      "source": [
        "# Balance the data there is no 3 rating thus we will not use 3 \r\n",
        "# every class will have 80,000 of data\r\n",
        "\r\n",
        "balanced_classes()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JKaOcC6uWaI"
      },
      "source": [
        "# Preprocess the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fu8d-4Caa7B5"
      },
      "source": [
        "# get the size of the new dataset to be read from file\r\n",
        "with open(file_b1, encoding='ISO-8859-1') as f:\r\n",
        "  new_dataset_size = len(f.readlines())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qe9NnflHgQjC",
        "outputId": "ecb1e8a4-6723-441f-dff5-42f881d041c1"
      },
      "source": [
        "new_dataset_size"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "320000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_u25j3efIcA"
      },
      "source": [
        "with open(file_b1, encoding='ISO-8859-1') as f:\r\n",
        "  da = f.readlines()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_HHnwk2fgf3"
      },
      "source": [
        "da[new_dataset_size-2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvbXGLXFFXbz"
      },
      "source": [
        "# We will create a tensorflow dataset using Textline dataset\r\n",
        "# TextLineDataset is usefull when creating a dataset from text files\r\n",
        "datasetX = tf.data.TextLineDataset([file_b1])\r\n",
        "datasety = tf.data.TextLineDataset([file_b2])\r\n",
        "\r\n",
        "# Combine the rating and review\r\n",
        "dataset = tf.data.Dataset.zip((datasetX, datasety))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnW19NnSo9j3"
      },
      "source": [
        "# Preprocess the data as follows:\r\n",
        "  # 1. Limit the number of characters to 300\r\n",
        "  # 2. Replace any special charcter with space\r\n",
        "  # 3. Replace any non alphabetical character with space\r\n",
        "  # 4. Split the batch to strings\r\n",
        "  # 5. As the data is read from file it is saved as byte array. Change this to number in y_batch\r\n",
        "  # 6. As the final output should be a Dense of 4 leave out 3 and shift rating values to start from 0\r\n",
        "  # 7. Finaly change X_batch to tensor and pad it  \r\n",
        "def preprocess(X_batch, y_batch):\r\n",
        "  X_batch = tf.strings.substr(X_batch, 0, 300)\r\n",
        "  X_batch = tf.strings.regex_replace(X_batch, b\"<br\\\\s*/?>\", b\" \")\r\n",
        "  X_batch = tf.strings.regex_replace(X_batch, b\"[^a-zA-Z']\", b\" \")\r\n",
        "  X_batch = tf.strings.split(X_batch)\r\n",
        "  y_batch = tf.strings.to_number(y_batch)\r\n",
        "  y_batch = tf.where(y_batch > 2, tf.subtract(y_batch, 2), tf.subtract(y_batch,1))\r\n",
        "  return X_batch.to_tensor(default_value=b\"<pad>\"), y_batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNnWxOw-UwRN"
      },
      "source": [
        "### Generate vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiQZnKhixwRf"
      },
      "source": [
        "# Get the list of words in our dataset\r\n",
        "from collections import Counter\r\n",
        "vocabulary = Counter()\r\n",
        "for X_batch, y_batch in dataset.batch(32).map(preprocess):\r\n",
        "  for review in X_batch:\r\n",
        "    value = review.numpy()\r\n",
        "    vocabulary.update(list(value))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6j5GM3DDVEN_"
      },
      "source": [
        "#### Truncate the vocabulary to a smaller size and create out of vocabulary set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFPbtaciGFJD"
      },
      "source": [
        "vocab_size = 20000\r\n",
        "truncated_vocabulary = [word for word,count in vocabulary.most_common(vocab_size)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHyGqCw2RFzC"
      },
      "source": [
        "num_oov = 2000\r\n",
        "words = tf.constant(truncated_vocabulary)\r\n",
        "word_ids = tf.range(vocab_size, dtype=tf.int64)\r\n",
        "vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\r\n",
        "table = tf.lookup.StaticVocabularyTable(vocab_init, num_oov)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNyoFFtQSIS7",
        "outputId": "18f56f93-1938-4454-eb61-87242f17e4dd"
      },
      "source": [
        "table.lookup(tf.constant([b'funny world'.split()]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 2), dtype=int64, numpy=array([[365, 193]])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4ODm5K6VU__"
      },
      "source": [
        "### Make the dataset ( train, valid, and test)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSDr8EY_aK5A"
      },
      "source": [
        "# Make train, valid and test datasets\r\n",
        "train_size = int(new_dataset_size*.9)\r\n",
        "test_size = int(new_dataset_size * .05)\r\n",
        "\r\n",
        "shuffled_dataset = dataset.shuffle(10000)\r\n",
        "train_set = shuffled_dataset.take(train_size)\r\n",
        "train_valid_set = shuffled_dataset.skip(train_size)\r\n",
        "\r\n",
        "test_set = train_valid_set.take(test_size)\r\n",
        "valid_set = train_valid_set.skip(test_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljKzJyVHhqBh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRFiTL6sVhgx"
      },
      "source": [
        "#### Encode the dataset with our lookup table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUqBgKJKSvMk"
      },
      "source": [
        "def encode_words(X_batch, y_batch):\r\n",
        "  return table.lookup(X_batch), y_batch\r\n",
        "\r\n",
        "train_set = train_set.batch(32).map(preprocess)\r\n",
        "train_set = train_set.map(encode_words).prefetch(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYJ0bvsrCTE3",
        "outputId": "375ce259-b2e7-4e40-9b1c-3ae5e2f28001"
      },
      "source": [
        "list(train_set.as_numpy_iterator())[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[  15, 7678, 7285, ...,    0,    0,    0],\n",
              "        [ 472,  682,   16, ...,    0,    0,    0],\n",
              "        [   5,   44,  145, ...,    0,    0,    0],\n",
              "        ...,\n",
              "        [  21,  826, 1665, ...,    0,    0,    0],\n",
              "        [   5,  145,    8, ...,    0,    0,    0],\n",
              "        [ 215,    5,  982, ...,    0,    0,    0]]),\n",
              " array([2., 2., 0., 3., 1., 0., 0., 3., 3., 0., 2., 3., 2., 0., 0., 0., 0.,\n",
              "        2., 1., 2., 1., 1., 2., 0., 2., 3., 0., 2., 0., 1., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 1., 0., 0., 3., 3., 1., 0., 1., 0., 3., 0.,\n",
              "        3., 0., 2., 0., 3., 1., 2., 3., 0., 3., 3., 0., 1.], dtype=float32))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qn3Q4FjVn0y"
      },
      "source": [
        "# Create a model and train, and save "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JY84KMTaTGKi",
        "outputId": "df7c870d-4434-416e-cb5c-020875d2d6f1"
      },
      "source": [
        "embed_size = 128\r\n",
        "model = tf.keras.Sequential([\r\n",
        "  tf.keras.layers.Embedding(vocab_size + num_oov, embed_size, input_shape=[None], mask_zero=True),\r\n",
        "  tf.keras.layers.GRU(128, return_sequences=True),\r\n",
        "  tf.keras.layers.GRU(128),\r\n",
        "  tf.keras.layers.Dense(4, activation='sigmoid')\r\n",
        "])\r\n",
        "model.compile(\r\n",
        "    optimizer=tf.keras.optimizers.Nadam(learning_rate=0.001),\r\n",
        "    loss=tf.keras.losses.sparse_categorical_crossentropy,\r\n",
        "    metrics=tf.keras.metrics.sparse_categorical_accuracy)\r\n",
        "history = model.fit(train_set, epochs=10, callbacks=tf.keras.callbacks.ModelCheckpoint('/content/drive/MyDrive/multi_domain_dataset/model_1.h5'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "9000/9000 [==============================] - 1277s 141ms/step - loss: 1.0398 - sparse_categorical_accuracy: 0.5337\n",
            "Epoch 2/10\n",
            "9000/9000 [==============================] - 1351s 150ms/step - loss: 0.7713 - sparse_categorical_accuracy: 0.6728\n",
            "Epoch 3/10\n",
            "9000/9000 [==============================] - 1332s 148ms/step - loss: 0.6057 - sparse_categorical_accuracy: 0.7510\n",
            "Epoch 4/10\n",
            "9000/9000 [==============================] - 1309s 145ms/step - loss: 0.4818 - sparse_categorical_accuracy: 0.8069\n",
            "Epoch 5/10\n",
            "9000/9000 [==============================] - 1297s 144ms/step - loss: 0.3798 - sparse_categorical_accuracy: 0.8528\n",
            "Epoch 6/10\n",
            "9000/9000 [==============================] - 1308s 145ms/step - loss: 0.2975 - sparse_categorical_accuracy: 0.8873\n",
            "Epoch 7/10\n",
            "9000/9000 [==============================] - 1298s 144ms/step - loss: 0.2297 - sparse_categorical_accuracy: 0.9157\n",
            "Epoch 8/10\n",
            "9000/9000 [==============================] - 1277s 142ms/step - loss: 0.1791 - sparse_categorical_accuracy: 0.9355\n",
            "Epoch 9/10\n",
            "9000/9000 [==============================] - 1280s 142ms/step - loss: 0.1402 - sparse_categorical_accuracy: 0.9504\n",
            "Epoch 10/10\n",
            "9000/9000 [==============================] - 1328s 148ms/step - loss: 0.1129 - sparse_categorical_accuracy: 0.9609\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3jktGfxcEyO"
      },
      "source": [
        "test = table.lookup(tf.constant(b'disgusted'.split()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCqy5oY-lHBu",
        "outputId": "6c8d93d6-706d-4425-a608-e623550637a0"
      },
      "source": [
        "model.predict(test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.93728346, 0.913895  , 0.15867427, 0.06335643]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LW_qwjnpNu5"
      },
      "source": [
        "# Using nnlm-en-50 sentence embedding module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vQrz7nApUvs"
      },
      "source": [
        "import tensorflow_hub as hub\r\n",
        "new_model = tf.keras.Sequential([\r\n",
        "  hub.KerasLayer(\"https://tfhub.dev/google/nnlm-en-dim50/2\",\r\n",
        "                      dtype=tf.string, input_shape=[], output_shape=[50]),\r\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\r\n",
        "  tf.keras.layers.Dense(4, activation='sigmoid')\r\n",
        "])\r\n",
        "new_model.compile(optimizer='adam', loss=tf.keras.losses.sparse_categorical_crossentropy,\r\n",
        "                  metrics=tf.keras.metrics.sparse_categorical_accuracy)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3gKYiAmqumx"
      },
      "source": [
        "def new_preprocess(X_batch, y_batch):\r\n",
        "  X_batch = tf.strings.substr(X_batch, 0, 300)\r\n",
        "  y_batch = tf.strings.to_number(y_batch)\r\n",
        "  y_batch = tf.where(y_batch > 2, tf.subtract(y_batch, 2), tf.subtract(y_batch,1))\r\n",
        "  return X_batch, y_batch\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nH-c3UcqiNV"
      },
      "source": [
        "new_dataset = shuffled_dataset.take(train_size)\r\n",
        "new_dataset = new_dataset.batch(64).map(new_preprocess)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlhzjIZ6rZcN"
      },
      "source": [
        "new_model.fit(new_dataset, epochs=200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNirzRw8RP6N"
      },
      "source": [
        "# Using Universal Sentence Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6AIPstxRUEx"
      },
      "source": [
        "from absl import logging\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "import os\r\n",
        "import pandas as pd\r\n",
        "import re\r\n",
        "import seaborn as sns\r\n",
        "\r\n",
        "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\r\n",
        "model = hub.load(module_url)\r\n",
        "print (\"module %s loaded\" % module_url)\r\n",
        "def embed(input):\r\n",
        "  return model(input)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijtu4nXuSVYM"
      },
      "source": [
        "#### From tensorflow hub tutorials"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxPkRXXxR4QZ",
        "outputId": "82b9b868-6005-484f-fb75-23cf455b80a8"
      },
      "source": [
        "word = \"Elephant\"\r\n",
        "sentence = \"I am a sentence for which I would like to get its embedding.\"\r\n",
        "paragraph = (\r\n",
        "    \"Universal Sentence Encoder embeddings also support short paragraphs. \"\r\n",
        "    \"There is no hard limit on how long the paragraph is. Roughly, the longer \"\r\n",
        "    \"the more 'diluted' the embedding will be.\")\r\n",
        "messages = [word, sentence, paragraph]\r\n",
        "\r\n",
        "# Reduce logging output.\r\n",
        "logging.set_verbosity(logging.ERROR)\r\n",
        "\r\n",
        "message_embeddings = embed(messages)\r\n",
        "\r\n",
        "for i, message_embedding in enumerate(np.array(message_embeddings).tolist()):\r\n",
        "  print(\"Message: {}\".format(messages[i]))\r\n",
        "  print(\"Embedding size: {}\".format(len(message_embedding)))\r\n",
        "  message_embedding_snippet = \", \".join(\r\n",
        "      (str(x) for x in message_embedding[:3]))\r\n",
        "  print(\"Embedding: [{}, ...]\\n\".format(message_embedding_snippet))\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:7 out of the last 7 calls to <function recreate_function.<locals>.restored_function_body at 0x7f3e99b52a70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:7 out of the last 7 calls to <function recreate_function.<locals>.restored_function_body at 0x7f3e99b52a70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Message: Elephant\n",
            "Embedding size: 512\n",
            "Embedding: [0.008344486355781555, 0.00048085825983434916, 0.06595248728990555, ...]\n",
            "\n",
            "Message: I am a sentence for which I would like to get its embedding.\n",
            "Embedding size: 512\n",
            "Embedding: [0.050808604806661606, -0.01652429811656475, 0.01573782227933407, ...]\n",
            "\n",
            "Message: Universal Sentence Encoder embeddings also support short paragraphs. There is no hard limit on how long the paragraph is. Roughly, the longer the more 'diluted' the embedding will be.\n",
            "Embedding size: 512\n",
            "Embedding: [-0.02833269163966179, -0.0558621808886528, -0.012941432185471058, ...]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWjWR3oASlK6"
      },
      "source": [
        "## Now lets use this embedder (encoder) in our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0U8urrg1Sr82"
      },
      "source": [
        "third_model = tf.keras.Sequential([\r\n",
        "  hub.KerasLayer('https://tfhub.dev/google/universal-sentence-encoder/4',\r\n",
        "                  dtype=tf.string, input_shape=[], output_shape=[512]),\r\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\r\n",
        "  tf.keras.layers.Dense(4, activation='softmax')\r\n",
        "])\r\n",
        "third_model.compile(optimizer='adam', loss=tf.keras.losses.sparse_categorical_crossentropy,\r\n",
        "                  metrics=tf.keras.metrics.sparse_categorical_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLKTliz4S14Y"
      },
      "source": [
        "third_model.fit(new_dataset, epochs=10)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}